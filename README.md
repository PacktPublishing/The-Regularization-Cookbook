# The-Regularization-Cookbook
The Regularization Cookbook, published by Packt


<details>
    <summary>
        <a href="chapter_01/chapter_01.ipynb">Chapter 1: An Overview of Regularization</a>
    </summary>

- Introducing regularization
</details>

<details>
    <summary>
        <a href="chapter_02/chapter_02.ipynb">Chapter 2: Machine Learning Refresher</a>
    </summary>

- Loading the data
- Splitting the data
- Preparing quantitative data
- Preparing qualitative data
- Model training
- Model evaluation
- Hyperparameter optimization
</details>

<details>
    <summary>
        <a href="chapter_03/chapter_03.ipynb">Chapter 3: Regularization with Linear Models</a>
    </summary>

- Training a Linear Regression with scikit-learn
- Regularizing with Ridge Regression
- Regularizing with Lasso Regression
- Regularizing with an Elastic Net Regression
- Training a Logistic Regression
- Regularizing a Logistic Regression
- Choosing the Right Regularization
</details>

<details>
    <summary>
        <a href="chapter_04/chapter_04.ipynb">Chapter 4: Regularization with Tree-based Models</a>
    </summary>

- Building a classification tree
- Building a Regression Tree
- Regularizing a decision tree
- Training a Random Forest
- Regularizing a Random Forest
- Training a Boosting model with XGBoost
- Regularizing with XGBoost
</details>

<details>
    <summary>
        <a href="chapter_05/chapter_05.ipynb">Chapter 5: Regularization with Data</a>
    </summary>

- Hashing high cardinality features
- Aggregating features
- Undersampling an imbalanced dataset
- Oversampling an imbalanced dataset
- Resampling imbalanced data with SMOTE
</details>

<details>
    <summary>
        <a href="chapter_06/chapter_06.ipynb">Chapter 6: Deep Learning Reminders</a>
    </summary>

- Training a perceptron
- Training a neural network for regression
- Training a neural network for binary classification
- Training a multiclass classification neural network
</details>

<details>
    <summary>
        <a href="chapter_07/chapter_07.ipynb">Chapter 7: Deep Learning Regularization</a>
    </summary>

- Regularizing a neural network with L2 regularization
- Regularizing a neural network with early stopping
- Regularizing with network architecture
- Regularizing with dropout
</details>

<details>
    <summary>
        <a href="chapter_08/chapter_08.ipynb">Chapter 8: Regularization with Recurrent Neural Networks</a>
    </summary>

- Training a RNN
- Training a GRU
- Regularizing with dropout
- Regularizing with maximum sequence length
</details>

<details>
    <summary>
        <a href="chapter_09/chapter_09.ipynb">Chapter 9: Advanced Regularization in Natural Language Processing</a>
    </summary>

- Regularization using a word2vec embedding
- Data augmentation using word2vec
- Zero-shot inference with pre-trained models
- Regularization with BERT embeddings
- Data augmentation using GPT-3
</details>

<details>
    <summary>
        <a href="chapter_10/chapter_10.ipynb">Chapter 10: Regularization in Computer Vision</a>
    </summary>

- Training a CNN
- Regularizing a CNN with vanilla NN methods
- Regularizing a CNN with transfer learning for object detection
- Semantic segmentation using transfer learning
</details>

<details>
    <summary>
        <a href="chapter_11/chapter_11.ipynb">Chapter 11: Regularization in Computer Vision: Synthetic Image Generation</a>
    </summary>

- Applying image augmentation with Albumentations
- Creating synthetic images for object detection
- Implementing real-time style transfer
</details>


# Usage

Clone this repo:
```shell
git clone git@github.com:PacktPublishing/The-Regularization-Cookbook.git
```

Create a virtual environment and install dependencies:
```shell
conda create -n regularization python=3.9
conda activate regularization
pip install -r requirements.txt
```

Then launch jupyter notebook, run and adapt the codes as you want:
```shell
jupyter notebook
```
